# RoboCap: Natural Language Scene Descriptions for Robots

System that generates natural language descriptions from images of "what a robot sees". (CV => NLP)

This idea aligns with real robotics applications such as assistance robots, autonomous navigation, and systems that need to interpret their environment in a way that is understandable to humans.

COCO dataset and a multimodal CV + NLP pipeline its used to train, validate and test the models.

## Motivation:
Robots often perceive their enviroment through cameras, but lack the ability to summarize visual information in language.

Natural-language descriptions give robots:
- Human friendly explanations of what they see.
- Context for decision making.
- Improved interpretability in assistive or domestic settings.
- A bridge between vision and communication.

The system aims to demonstrate how image captioning can serve as a foundational module for robot awareness.

## Dataset:
As it was established, the sysytem uses the COCO dataset (Common Objects in Context), which contains:
- 118k training images.
- 5k validation images.
- 5 captions per image.

Source Link: https://cocodataset.org/#download

## Expected Outcomes of the project (Before development):
By the end of the project, the system RoboCap will:
- Generate readable captions from unseen images.
- Highlight actions and objects relevant to robot navigation.

## Initialation steps:
<text...>

# Project developed by:
- Andrés Jaramillo Barón | A01029079
- Pedro Mauri Martínez | A01029143
